{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Tree or Other Algorithm Exercise\n",
    "\n",
    "Kaggle hosts a dataset which contains house sales prices for King County, which includes Seattle. \n",
    "\n",
    "You can download the dataset from [Kaggle](https://www.kaggle.com/harlfoxem/housesalesprediction) or feel free to download it from my [GitHub](https://raw.githubusercontent.com/mGalarnyk/Tutorial_Data/master/King_County/kingCountyHouseData.csv)\n",
    "\n",
    "\n",
    "Your goal is to do the following:\n",
    "\n",
    "1. The challenge is about predicting house prices based on whatever features in the dataset you choose. One thing to keep in mind is if you dont know what the features in the dataset mean, you can look on Kaggle for the documentation (you dont need an account to view feature information). \n",
    "2. Do some exploratory data analysis.\n",
    "3. For this notebook, use cross validation and grid search. While I haven't showed the code for how to do this in the course, spend some time figuring it out. \n",
    "\n",
    "\n",
    "For this paritcular notebook you need to install folium if you want to run all the cells. \n",
    "\n",
    "Option 1:\n",
    "`pip install folium`\n",
    "\n",
    "Option 2 (Anaconda):\n",
    "`conda install -c conda-forge folium`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn import tree\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import folium\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import make_scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/mGalarnyk/Tutorial_Data/master/King_County/kingCountyHouseData.csv'\n",
    "df = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis\n",
    "This is not a very big dataset and we do not have too many features. Creating plots and examining the data before applying a model is a very good practice because we may find some outliers or decide to do normalize the data. This is not a must but getting to know the data is always good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at how many rows are in the dataset\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histograms for Continuous Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continous_columns = ['price',\n",
    "                      'bedrooms',\n",
    "                      'bathrooms',\n",
    "                      'sqft_living',\n",
    "                      'sqft_lot',\n",
    "                      'floors',\n",
    "                      'waterfront',\n",
    "                      'view',\n",
    "                      'condition',\n",
    "                      'grade',\n",
    "                      'sqft_above',\n",
    "                      'sqft_basement',\n",
    "                      'yr_built',\n",
    "                      'yr_renovated',\n",
    "                      'zipcode',\n",
    "                      'lat',\n",
    "                      'long',\n",
    "                      'sqft_living15',\n",
    "                      'sqft_lot15']\n",
    "df.loc[:,continous_columns].hist(bins=25,figsize=(16,16),xlabelsize='10',ylabelsize='10',xrot=-15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To look at bedrooms, floors, bathrooms, and other variables vs price, I prefer boxplots because we have numerical data that is mostly not continuous. If you are curious what a boxplot is, I have an article on it [here](https://towardsdatascience.com/understanding-boxplots-5e2df7bcbd51).\n",
    "\n",
    "From the charts below, it can be seen that there are some outliers like 33 bedrooms for a house and a price around 7000000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows = 3,\n",
    "                         ncols = 1,\n",
    "                         dpi=1000)\n",
    "\n",
    "sns.boxplot(x=df['bedrooms'],y=df['price'], ax=axes[0], showfliers = False)\n",
    "#axes[0].set_ylabel('')\n",
    "sns.boxplot(x=df['floors'],y=df['price'], ax=axes[1], showfliers = False)\n",
    "#axes[1].set_ylabel('')\n",
    "sns.boxplot(x=df['bathrooms'],y=df['price'], ax=axes[2], showfliers = False)\n",
    "axes[2].tick_params(axis = 'x', rotation = 90)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows = 3,\n",
    "                         ncols = 1,\n",
    "                         dpi=1000)\n",
    "\n",
    "sns.boxplot(x=df['waterfront'],y=df['price'], ax=axes[0], showfliers = False)\n",
    "sns.boxplot(x=df['view'],y=df['price'], ax=axes[1], showfliers = False)\n",
    "sns.boxplot(x=df['grade'],y=df['price'], ax=axes[2], showfliers = False)\n",
    "axes[1].tick_params(axis = 'x', rotation = 90)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this dataset, we have latitude and longtitude information for the houses. By using lat and long columns, I created the map below using the folium library which is a wrapper of a javascript library called [leaflet](https://leafletjs.com/reference-1.6.0.html#circlemarker-option). Notice that I didnt have equal sized bins because splitting the data into equal sized bins when the greatest house is 7,700,000 million and the least is 75,000 into even 6 bins means that each bin would cover more than a million. In the map, I didnt add a legend though I probably should ([link](https://stackoverflow.com/questions/37466683/create-a-legend-on-a-folium-map) to learn how to do it if curious)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick sidenote, look at the min and max price of home as well as most amount of bedrooms\n",
    "df['price'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['price'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['price'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram for price\n",
    "(n, bins, patches) = plt.hist(df['price'].values,\n",
    "                              bins=6,\n",
    "                              edgecolor='black',\n",
    "                              linewidth=.9)\n",
    "plt.tick_params(axis = 'x', rotation = 90, labelsize = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The edges of the bins.\n",
    "bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram for bedrooms\n",
    "(n, bins, patches) = plt.hist(df['bedrooms'].values,\n",
    "                              bins=6,\n",
    "                              edgecolor='black',\n",
    "                              linewidth=.9)\n",
    "plt.tick_params(axis = 'x', rotation = 90, labelsize = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The edges of the bins. Having bins this size is ridiculous. Should I have 33 bins?\n",
    "bins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bin the histogram into quartiles so we can have some more balanced bins and reasonable colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles = df['price'].quantile([0,0.01, 0.25, 0.5, 0.75, 0.99, 1])\n",
    "df['price_bin'] = pd.cut(df['price'], bins = quantiles.values)\n",
    "\n",
    "# Removing left most house (cheapest)\n",
    "df = df.loc[~df['price_bin'].isna(), :]\n",
    "df['price_left'] = df['price_bin'].apply(lambda x: x.left)\n",
    "\n",
    "# Making color based on quantiles rather than equal size bins. \n",
    "hex_dict = {}\n",
    "for index,left in enumerate(df['price_left'].value_counts().sort_index().index.values):\n",
    "    hex_dict[left] = sns.color_palette(\"RdBu\", 6).as_hex()[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting the colors as hex because folium doesnt take tuples as inputs (for unknown reason)\n",
    "hex_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# You need to have installed folium to make this work\n",
    "# Creating Map\n",
    "startingmap = folium.Map(location=[47.5112, -122.257], control_scale=True, zoom_start=9.4)\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    \n",
    "    \n",
    "    price = int(row['price'])\n",
    "    bedrooms = row['bedrooms']\n",
    "    floors = row['floors']\n",
    "    bathrooms = row['bathrooms']\n",
    "    living = row['sqft_living']\n",
    "    waterfront = row['waterfront']\n",
    "    \n",
    "    popupinformation = ('Price: ' + \"{:,}\".format(price) + '<br>'\n",
    "                        'Bedrooms: ' + str(bedrooms) + '<br>'\n",
    "                        'Floors: ' + str(floors) + '<br>'\n",
    "                        'Bathrooms: ' + str(bathrooms) + '<br>'\n",
    "                        'Sqft_Living: ' + str(living) + '<br>'\n",
    "                        'Waterfront: ' + str(waterfront) + '<br>'\n",
    "                       )\n",
    "    \n",
    "    folium.CircleMarker([row['lat'], row['long']],\n",
    "                        color = hex_dict[row['price_left']],\n",
    "                        weight = .5,\n",
    "                        fill = True,\n",
    "                        fillColor = hex_dict[row['price_left']],\n",
    "                        popup = popupinformation,\n",
    "                        opacity = .3,\n",
    "                        fillOpacity = .3).add_to(startingmap)\n",
    "    \n",
    "startingmap.save('seattleMap.html')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is clearly a relationship between location and price in this dataset but can a model that we build capture that. If we really want to make a good prediction, we could include additional information like schools in the area (like zillow) among many other things (distance to companies, more information on the homes). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Multiple Features\n",
    "One  benefit of modeling is the ability to reason about hundreds of features at once. There is no limit to the number of features you can use. However, often a small set of features accounts for most of the variance (assuming there is a linear relationship at all). A relatively good way to choose features is to plot a correlation matrix (though with a lot of variables, the matrix can be a bit overwhelming)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approach 1 to view the correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr().sort_values(by = ['price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use a heat map to make it easier (in theory) to read the correlation matrix.\n",
    "sns.heatmap(df.corr().sort_values(by = ['price']), cmap = sns.diverging_palette(240, 10, n=9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approach 2 to view the correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing not all features right now\n",
    "\n",
    "features = ['price','bedrooms','bathrooms','sqft_living','sqft_lot','floors','waterfront',\n",
    "            'view','condition','grade','sqft_above','sqft_basement','yr_built','yr_renovated',\n",
    "            'zipcode','lat','long','sqft_living15','sqft_lot15']\n",
    "\n",
    "mask = np.zeros_like(df[features].corr(), dtype=np.bool) \n",
    "mask[np.triu_indices_from(mask)] = True \n",
    "\n",
    "f, ax = plt.subplots(figsize=(16, 12))\n",
    "plt.title('Pearson Correlation Matrix',fontsize=25)\n",
    "\n",
    "sns.heatmap(df[features].corr(),linewidths=0.25,vmax=0.7,square=True,cmap=\"RdBu\", \n",
    "            linecolor='w',annot=True,annot_kws={\"size\":8},mask=mask,cbar_kws={\"shrink\": .9});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arrange Data into Features Matrix and Target Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Picked some features for now\n",
    "features = ['bedrooms','bathrooms','sqft_living','sqft_lot','floors']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.loc[:, features]\n",
    "\n",
    "y = df.loc[:, 'price'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data into Training and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a Model to Show What we Have to Improve Upon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an instance of the Model.\n",
    "reg = DecisionTreeRegressor()\n",
    "\n",
    "# Training the model on the data, storing the information learned from the data\n",
    "reg.fit(X_train, Y_train)\n",
    "\n",
    "### Measure Model Performance\n",
    "score = reg.score(X_test, Y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.get_depth()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune the Depth of a Tree\n",
    "Finding the optimal value for max_depth is one way to tune your model. The code below outputs the R^2 for regression trees with different values for max_depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of values to try for n_estimators:\n",
    "max_depth_range = list(range(1,80))\n",
    "\n",
    "# List to store the R2 for each value of max_depth\n",
    "score_list = []\n",
    "\n",
    "for depth in max_depth_range:\n",
    "    reg = DecisionTreeRegressor(max_depth = depth)\n",
    "    reg.fit(X_train, Y_train)\n",
    "    score = reg.score(X_test, Y_test)\n",
    "    score_list.append(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the graph below shows that the best accuracy for the model is when the parameter max_depth is greater than or equal to 3, it might be best to choose the least complicated model with max_depth = 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows = 1, ncols = 1, figsize = (10,7));\n",
    "ax.plot(max_depth_range,\n",
    "        score_list,\n",
    "        lw=2,\n",
    "        color='k')\n",
    "ax.set_xlim([1, max(max_depth_range)])\n",
    "ax.grid(True,\n",
    "        axis = 'both',\n",
    "        zorder = 0,\n",
    "        linestyle = ':',\n",
    "        color = 'k')\n",
    "ax.tick_params(labelsize = 18)\n",
    "ax.set_xlabel('max_depth', fontsize = 24)\n",
    "ax.set_ylabel('R^2', fontsize = 24)\n",
    "fig.tight_layout()\n",
    "#fig.savefig('images/max_depth_vs_R2.png', dpi = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not an ideal approach as you will see in the next section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning a Machine Learning Model Via Grid Search\n",
    "In machine learning, we have two types of parameters. One are those learned from the training data. For example, weights for linear regression. The second are parameters of a learning algorithm that are optimized separately.The latter are tuning parameters, also known as hyperparameters. The code we will use evaluates the optimal combination of hyperparameter values using grid search. \n",
    "\n",
    "Grid search is a brute force exhaustive search paradigm where we specify a list of values for different hyperparameters, and the grid search algorithm evaluates the model performance for each combination to obtain the optimal combination of values from this set. If we did this with a normal train test split, we would be essentially reusing the same test dataset over and over again. This is a problem as a test set will become part of our training data and a model we choose will be more likely to overfit. \n",
    "\n",
    "One approach would be to split our dataset into three parts:a training dataset, validation set, and a test set. The training dataset is used to fit the different models, and the performance on the validation dataset is then used for model selection. The following figure illustrates the concept of holdout cross-validation, where you use a validation dataset to repeatedly evaluate the performance of the model after training using different hyperparameter values. Once we are satisfied with the tuning of hyperparameter values, we estimate the model's generalization performance on the test dataset. \n",
    "\n",
    "![images](images/hyperparametersRepeat.png)\n",
    "Image from [Python Machine Learning](https://github.com/rasbt/python-machine-learning-book-3rd-edition) pg 196.\n",
    "\n",
    "What scikit-learn \n",
    "([code](https://github.com/scikit-learn/scikit-learn/blob/95d4f0841/sklearn/model_selection/_search.py#L841), [documentation](https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation)) does for GridSearchCV is more similar to the image below. \n",
    "\n",
    "![images](images/crossvalidationidea.png)\n",
    "\n",
    "An image of k-fold cross-validation used by scikit-learn for this process is illustrated below. The performance measure reported by k-fold cross-validation is the average of the values computed in the loop. This approach can be computationally expensive, but does not waste too much data (as is the case when fixing an arbitrary validation set).\n",
    "\n",
    "![images](images/kfoldcrossvalidation.png)\n",
    "\n",
    "The code below is only looking for the optimal `max_depth`, but in the future we will likely use grid search with alot more hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(range(1,20)) + [None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intitialize a GridSearchCV object and specify we want to tune max_depth\n",
    "gs = GridSearchCV(estimator=DecisionTreeRegressor(random_state = 0),\n",
    "                  param_grid = [{'max_depth': list(range(1,20)) + [None]}],\n",
    "                  cv = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is computationally expensive\n",
    "pd.DataFrame(gs.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best performing model's score\n",
    "print(gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best performing model's parameters\n",
    "print(gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best performing model\n",
    "reg = gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use an independent test to evaluate the performance of the best performing model. \n",
    "reg.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a score for the testing set\n",
    "reg.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning a Machine Learning Model Via Randomized Search\n",
    "As mentioned earlier, grid search is computationally expensive. [Randomized search](http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf) usually performs about as well as grid search but is much more cost and time effective. In contrast to GridSearchCV, RandomizedSearchV doesn't try out not all parameter values, but rather a fixed number of parameter settings is sampled from the specified distributions. The number of parameter settings that are tried is given by n_iter. We aren't going to cover how it works, but I mention this as over time so many advances have made machine learning more acccessible over time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intitialize a GridSearchCV object and specify we want to tune max_depth\n",
    "rs = RandomizedSearchCV(estimator=DecisionTreeRegressor(random_state = 0),\n",
    "                        param_distributions = {'max_depth': list(range(1,20)) + [None]},\n",
    "                        cv = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is less computationally expensive\n",
    "pd.DataFrame(rs.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best performing model's score\n",
    "print(rs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best performing model's parameters\n",
    "print(rs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best performing model\n",
    "reg = rs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use an independent test to evaluate the performance of the best performing model. \n",
    "reg.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a score for the testing set\n",
    "reg.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you had to make this model better, what are some things we can do to make this model better? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create or Import your Own Scoring Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_metric(y_true, y_predict):\n",
    "    \"\"\" Calculates and returns the performance score between \n",
    "        true and predicted values based on the metric chosen. \"\"\"\n",
    "\n",
    "    #Calculate the performance score between 'y_true' and 'y_predict'\n",
    "    score = mean_squared_error(y_true, y_predict)\n",
    "\n",
    "    # Return the score\n",
    "    return score\n",
    "\n",
    "scoring_fnc = make_scorer(performance_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intitialize a GridSearchCV object and specify we want to tune max_depth\n",
    "rs = RandomizedSearchCV(estimator=DecisionTreeRegressor(random_state = 0),\n",
    "                        param_distributions = {'max_depth': list(range(1,20)) + [None]},\n",
    "                        scoring=scoring_fnc,\n",
    "                        cv = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is less computationally expensive\n",
    "pd.DataFrame(rs.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best performing model's score\n",
    "print(rs.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not the most understandable metric. People often use R^2 as it is basically rescaled version of MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best performing model's parameters\n",
    "print(rs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best performing model\n",
    "reg = rs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use an independent test to evaluate the performance of the best performing model. \n",
    "reg.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a score for the testing set\n",
    "reg.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Didnt seem like this worked out well. Just showed this to mention that you can make your own metric. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying Multiple Machine Learning Models\n",
    "A future task we will look into for future classes is to make a table of different models and their scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
